{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3-cqvdhifiIg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyz4fw4ZHW9E"
      },
      "source": [
        "Reading the datasets.\n",
        "Applying label encoder and then concatenate the two datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S00HQshNBECD",
        "outputId": "5cc66d3e-60a7-450c-c711-20bfea8722e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tweets   labels\n",
            "0  RT @Papapishu: Man it would fucking rule if we...  hateful\n",
            "1  It is time to draw close to Him &#128591;&#127...   normal\n",
            "2  if you notice me start to act different or dis...   normal\n",
            "3  Forget unfollowers, I believe in growing. 7 ne...   normal\n",
            "4  RT @Vitiligoprince: Hate Being sexually Frustr...  hateful\n",
            "5  Topped the group in TGP Disc Jam Season 2! Ont...   normal\n",
            "6  That daily baby aspirin for your #heart just m...   normal\n",
            "7  I liked a @YouTube video from @mattshea https:...   normal\n",
            "8  RT @LestuhGang_: If your fucking up &amp; your...  hateful\n",
            "9  @Move_Fwd give up. You've lost. You will not c...   normal\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = pd.read_excel('hatespeech_text.xlsx', header = None)\n",
        "df.rename(columns={0:'tweets', 1:'labels'}, inplace=True)\n",
        "print(df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kc6jTUWkHW9I",
        "outputId": "a75e626e-096b-4301-87fc-21033a7c2d38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              tweets  labels\n",
              "0  RT @Papapishu: Man it would fucking rule if we...       0\n",
              "1  It is time to draw close to Him &#128591;&#127...       1\n",
              "2  if you notice me start to act different or dis...       1\n",
              "3  Forget unfollowers, I believe in growing. 7 ne...       1\n",
              "4  RT @Vitiligoprince: Hate Being sexually Frustr...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5e0486a-f0a2-4c27-be2c-2e9dd99ce404\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>if you notice me start to act different or dis...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5e0486a-f0a2-4c27-be2c-2e9dd99ce404')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5e0486a-f0a2-4c27-be2c-2e9dd99ce404 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5e0486a-f0a2-4c27-be2c-2e9dd99ce404');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "df['labels']=le.fit_transform(df['labels'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XOoCkodDHW9J",
        "outputId": "d68b10f7-2991-457e-c8ce-b5ea80e32771"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              tweets  labels\n",
              "0                             lmfaoo  üò≠  üò≠  üò≠  üò≠  üò≠        0\n",
              "1                            i hate this feeling  üò¢        0\n",
              "2  can't believe i just went out in this cold to ...       0\n",
              "3  i need a new trap house, so if you really fuck...       0\n",
              "4            <user> so very sorry for your loss.  üíî        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f13edb59-08ac-4a5b-99ee-e4e629ccb68f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lmfaoo  üò≠  üò≠  üò≠  üò≠  üò≠</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i hate this feeling  üò¢</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>can't believe i just went out in this cold to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i need a new trap house, so if you really fuck...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;user&gt; so very sorry for your loss.  üíî</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f13edb59-08ac-4a5b-99ee-e4e629ccb68f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f13edb59-08ac-4a5b-99ee-e4e629ccb68f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f13edb59-08ac-4a5b-99ee-e4e629ccb68f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "de=pd.read_pickle(\"emoji_tweets.pkl\")\n",
        "emoji = pd.DataFrame.from_dict(de)\n",
        "emoji.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apQMlg0tHW9K"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([df,emoji])\n",
        "data.head()\n",
        "data.tweets=data.tweets.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChnjH8kzHW9L"
      },
      "source": [
        "Pre Processing\n",
        " 1-> For hate speech detection as the dataset contains tweets along with the actual tweets some of them include the Twitter handle of people, hashtags, links to certain websites or the tweet itself\n",
        "First, we replace all tags and hashtags with space using regex (@ [^\\s] + which means replace anything which @ accompanied by anything but a space). Then we replace all the website links with a space.\n",
        "For hate speech detection as the dataset contains tweets along with the actual tweets some of them include the Twitter handle of people, hashtags, links to certain websites or the tweet itself and emoticons. We add rt to stopwords which represents retweet. First, we replace all tags and hashtags with space using regex (@ [^\\s] + which means replace anything which @ accompanied by anything but a space). Then we replace all the website links with a space.\n",
        "Then we split the sentence into words and check if the word is present in stopwords we remove it and then again join the sentence. We then replace multiple spaces with a single space, strip the sentences for extra trailing spaces. we replace nan (not a number) values with a space and finally drop any missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtX0VBfBf9H1",
        "outputId": "6cdc730d-cda5-49f5-9c8f-e949d2aac57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-6-5c6ab3fd789f>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweets\"] = data[\"tweets\"].str.replace(' +', ' ', case=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tweets  labels\n",
            "0    man would fucking rule party perpetual warfare.       0\n",
            "1      time draw close & father, draw near always ‚ù§Ô∏è       1\n",
            "2  notice start act different distant.. bc peeped...       1\n",
            "3  forget unfollowers, believe growing. 7 new fol...       1\n",
            "4  hate sexually frustrated like wanna fuck ion w...       0\n",
            "5  topped group tgp disc jam season 2! onto semi-...       1\n",
            "6     daily baby aspirin might preventing colon too.       1\n",
            "7  liked video blue army coming! - ancient warfare 2       1\n",
            "8  fucking &amp; homies dont tell fucking up, ain...       0\n",
            "9  give up. lost. convince one iota read conspira...       1\n",
            "                                                  tweets  labels\n",
            "13190  clear message. use helmet save life üëá \\n\\n<use...       1\n",
            "13191                         funny üòÜ wish could get one       1\n",
            "13192                        show respect, get respect ‚òù       1\n",
            "13193                              waahh im happy you! üíñ       1\n",
            "13194                               beautiful gorgeous üòò       1\n",
            "13195         love waking skinny ahaha wish lasted day üòÖ       1\n",
            "13196              magnificent pair tits üòç cock hard üçÜ üòÄ       1\n",
            "13197            soon mamsh üòò god give best among best üíñ       1\n",
            "13198                                          trust u üòé       1\n",
            "13199                                       aww thanks üòÅ       1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85966"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = stopwords.words('english')\n",
        "STOPWORDS.append(\"rt\")\n",
        "STOPWORDS.append(\"<user>\")\n",
        "STOPWORDS.append(\"<url>\")\n",
        "def clean_text():\n",
        "    data[\"tweets\"] = data[\"tweets\"].apply(lambda x: x.lower())\n",
        "    data[\"tweets\"] = [re.sub('(@[^\\s]+)|(#[^\\s]+)', '', tweet) for tweet in data[\"tweets\"]]\n",
        "    data[\"tweets\"] = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) for tweet in data[\"tweets\"]]\n",
        "    data[\"tweets\"] = data[\"tweets\"].str.split(' ').apply(lambda tweet: ' '.join(k for k in tweet if k not in STOPWORDS))\n",
        "    data[\"tweets\"] = data[\"tweets\"].str.replace(' +', ' ', case=False)\n",
        "    data[\"tweets\"] = data[\"tweets\"].str.strip()\n",
        "    data[\"tweets\"].replace('', np.nan, inplace=True)\n",
        "    df.dropna(subset=[\"tweets\"], inplace=True)\n",
        "      \n",
        "clean_text()\n",
        "    \n",
        "print(data.head(10))\n",
        "print(data.tail(10))\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCbdwmuwHW9N"
      },
      "source": [
        "For emoticon function first we tokenize each word of the sentence using word_tokenize. Then we check whether the word is a emoji or not if no we add if to sentence else we convert the emoji into its corresponding meaning (ie üòÄ turns to smile) using the emoji.demojize function imported from library emoji , convert the customary ':' sign attached to it to a space, remove extra spaces and then add it to the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl8VIhxeHW9O",
        "outputId": "fea012ae-3519-43f3-998d-f9e0a1da847e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240 kB 4.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=df7c3462a46569be8b1d670c6290e415f3d8ead4cc3c6cac0b1ed5227beaac86\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13195    love waking skinny ahaha wish lasted day  grin...\n",
              "13196    magnificent pair tits  smiling_face_with_heart...\n",
              "13197    soon mamsh  face_blowing_a_kiss  god give best...\n",
              "13198              trust u  smiling_face_with_sunglasses  \n",
              "13199         aww thanks  beaming_face_with_smiling_eyes  \n",
              "Name: tweets, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "!pip3 install emoji\n",
        "\n",
        "import emoji\n",
        "def emoticon(sentence):\n",
        "    words=word_tokenize(sentence)\n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        if emoji.demojize(word)== None:\n",
        "            stem_sentence.append(word)\n",
        "            stem_sentence.append(\" \")\n",
        "        else:\n",
        "            word= emoji.demojize(word)\n",
        "            word = word.replace(\":\",\" \")\n",
        "            stem_sentence.append(word)\n",
        "            stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "    \n",
        "training_size = 1000000        \n",
        "sentences = data.tweets.astype(str)\n",
        "sentences = sentences[:training_size].apply(emoticon)\n",
        "sentences.head()\n",
        "sentences.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnHSHXgHW9P"
      },
      "source": [
        "Here we convert our words into root words after analyzing its context in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynvox6T25wHf",
        "outputId": "148e450c-6546-4ef5-fe24-abcbccb38495"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gBbolNAHW9Q",
        "outputId": "63a0139f-490f-4ab3-ac90-127e0be3a7ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13195    love waking skinny ahaha wish lasted day grinn...\n",
              "13196    magnificent pair tit smiling_face_with_heart-e...\n",
              "13197    soon mamsh face_blowing_a_kiss god give best a...\n",
              "13198                trust u smiling_face_with_sunglasses \n",
              "13199           aww thanks beaming_face_with_smiling_eyes \n",
              "Name: tweets, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = LancasterStemmer()\n",
        "    words=word_tokenize(sentence)\n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(lemmatizer.lemmatize(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "training_size = 1000000\n",
        "sentences = sentences[:training_size].apply(stemSentence)\n",
        "sentences.head()\n",
        "sentences.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422x5WNfHW9Q"
      },
      "source": [
        "Splitting the dataset into 70:30 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvlivu42gDDg",
        "outputId": "ecc11815-da38-4986-d6d5-52f36cf6b553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total   tweet count: 99166\n",
            "Normal  tweet count: 60450\n",
            "Hateful tweet count: 38716 \n",
            "\n",
            "Total tweet count in training sampple: 69416\n",
            "Total tweet count in test sample:     29750\n",
            "Normal  tweet count in X_train: 42344\n",
            "Hateful tweet count in X_train: 27072\n",
            "Normal  tweet count in X_test:  18106\n",
            "Hateful tweet count in X_test:  11644\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data splitting\n",
        "X = sentences\n",
        "y = data[\"labels\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "\n",
        "print ('Total   tweet count:', len(data))\n",
        "print ('Normal  tweet count:', len(data[data.labels == 1]))\n",
        "print ('Hateful tweet count:', len(data[data.labels == 0]), '\\n')\n",
        "print ('Total tweet count in training sampple:', len(X_train))\n",
        "print ('Total tweet count in test sample:    ', len(X_test))\n",
        "print ('Normal  tweet count in X_train:', X_train[y_train == 1].count())\n",
        "print ('Hateful tweet count in X_train:', X_train[y_train == 0].count())\n",
        "print ('Normal  tweet count in X_test: ', X_test[y_test == 1].count())\n",
        "print ('Hateful tweet count in X_test: ', X_test[y_test == 0].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ti2-Aa-HW9R"
      },
      "source": [
        "Fitting and transforming tfidfvectorizer on training sentences and transforming testing sentences based on training sentences\n",
        "Create a pickle file containing tfidf fitted vectorizer so that we can call in our python file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9n-slF7dTus",
        "outputId": "cda2efa6-7bbf-4375-cf37-e626d74ec9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(69416, 49384)\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "cv = CountVectorizer()\n",
        "tfidftrans = TfidfVectorizer()\n",
        "X_train = tfidftrans.fit_transform(X_train)\n",
        "print(X_train.shape)\n",
        "pickle.dump(tfidftrans, open(\"tfidf.pickle\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb8zeWF1HW9S",
        "outputId": "26ad128f-8b3b-4ec8-d71d-09edd0ea3e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29750, 49384)\n"
          ]
        }
      ],
      "source": [
        "X_test = tfidftrans.transform(X_test)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOENHeFsHW9S"
      },
      "source": [
        "Feeding the data to MultinomialNB, KNN ,Logistic regression, Decision Tree\n",
        "Adding the accuracy scores to a dictionary to compare later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prlgOxtxHW9T",
        "outputId": "3ec37ca7-af57-4963-f7c1-a6086c2eb4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8883025210084033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85     11644\n",
            "           1       0.89      0.93      0.91     18106\n",
            "\n",
            "    accuracy                           0.89     29750\n",
            "   macro avg       0.89      0.88      0.88     29750\n",
            "weighted avg       0.89      0.89      0.89     29750\n",
            "\n",
            "[[ 9664  1980]\n",
            " [ 1343 16763]]\n"
          ]
        }
      ],
      "source": [
        "def training(clf,x_train,Y_train):\n",
        "    clf.fit(x_train,Y_train)\n",
        "def predict(clf,X_test):\n",
        "    return clf.predict(X_test)\n",
        "dict={}\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB(alpha =0.2)\n",
        "training(mnb,X_train,y_train)\n",
        "pred = predict(mnb,X_test)\n",
        "print(accuracy_score(y_test,pred,normalize=True))\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test,pred))\n",
        "dict[mnb]=accuracy_score(y_test,pred,normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGKvvdO6HW9T",
        "outputId": "e0130914-d63c-43f5-ddf5-ef12f6067b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k =  1\n",
            "0.7767394957983194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.48      0.63     11644\n",
            "           1       0.74      0.97      0.84     18106\n",
            "\n",
            "    accuracy                           0.78     29750\n",
            "   macro avg       0.82      0.72      0.73     29750\n",
            "weighted avg       0.80      0.78      0.76     29750\n",
            "\n",
            "[[ 5632  6012]\n",
            " [  630 17476]]\n",
            "k =  2\n",
            "0.776235294117647\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.50      0.63     11644\n",
            "           1       0.75      0.96      0.84     18106\n",
            "\n",
            "    accuracy                           0.78     29750\n",
            "   macro avg       0.81      0.73      0.74     29750\n",
            "weighted avg       0.80      0.78      0.76     29750\n",
            "\n",
            "[[ 5769  5875]\n",
            " [  782 17324]]\n",
            "k =  3\n",
            "0.7519663865546219\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.39      0.55     11644\n",
            "           1       0.72      0.98      0.83     18106\n",
            "\n",
            "    accuracy                           0.75     29750\n",
            "   macro avg       0.83      0.69      0.69     29750\n",
            "weighted avg       0.80      0.75      0.72     29750\n",
            "\n",
            "[[ 4572  7072]\n",
            " [  307 17799]]\n",
            "k =  4\n",
            "0.7589915966386555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.42      0.58     11644\n",
            "           1       0.72      0.97      0.83     18106\n",
            "\n",
            "    accuracy                           0.76     29750\n",
            "   macro avg       0.82      0.70      0.71     29750\n",
            "weighted avg       0.80      0.76      0.73     29750\n",
            "\n",
            "[[ 4940  6704]\n",
            " [  466 17640]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "for i in range(1,5):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    training(knn,X_train,y_train)\n",
        "    pred = predict(knn,X_test)\n",
        "    print('k = ',i)\n",
        "    print(accuracy_score(y_test,pred,normalize=True))\n",
        "    print(classification_report(y_test, pred))\n",
        "    print(confusion_matrix(y_test,pred))\n",
        "    dict[knn,i]=accuracy_score(y_test,pred,normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE49Yg4mHW9U",
        "outputId": "19e35cd8-4723-4d6c-ad4d-89c550490eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9172773109243697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.90      0.90     11644\n",
            "           1       0.94      0.93      0.93     18106\n",
            "\n",
            "    accuracy                           0.92     29750\n",
            "   macro avg       0.91      0.91      0.91     29750\n",
            "weighted avg       0.92      0.92      0.92     29750\n",
            "\n",
            "[[10508  1136]\n",
            " [ 1325 16781]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "training(dt,X_train,y_train)\n",
        "pred = predict(dt,X_test)\n",
        "print(accuracy_score(y_test,pred,normalize=True))\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test,pred))\n",
        "dict[dt]=accuracy_score(y_test,pred,normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyueun7gHW9U",
        "outputId": "10d779cb-8a60-4394-f86d-a625f2e5256f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9355294117647058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     11644\n",
            "           1       0.93      0.97      0.95     18106\n",
            "\n",
            "    accuracy                           0.94     29750\n",
            "   macro avg       0.94      0.93      0.93     29750\n",
            "weighted avg       0.94      0.94      0.94     29750\n",
            "\n",
            "[[10250  1394]\n",
            " [  524 17582]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "training(lr,X_train,y_train)\n",
        "pred = predict(lr,X_test)\n",
        "print(accuracy_score(y_test,pred,normalize=True))\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test,pred))\n",
        "dict[lr]=accuracy_score(y_test,pred,normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENS4s034HW9U"
      },
      "source": [
        "Finding model with highest accuracy and creating a pickle file of respective model to call in python file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoOcJQOiHW9V",
        "outputId": "b6c1971f-8246-459f-8125-e1dbc5d4cbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9355294117647058\n",
            "LogisticRegression(max_iter=10000)\n"
          ]
        }
      ],
      "source": [
        "n=max(dict.values())\n",
        "print(n)\n",
        "for name,predicted in dict.items():\n",
        "    if predicted==n:\n",
        "        print(name)\n",
        "        model=name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "e4CWclwBHW9V",
        "outputId": "7734dc12-cfe1-4e54-b1b8-9de7b818efbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/stride_tricks.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  array = np.array(array, copy=False, subok=subok)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "names = list(dict.keys())\n",
        "values = list(dict.values())\n",
        "plt.barh(range(len(dict)), values, tick_label=names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xYFB-pp7HW9V"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('hate_speech.pickle', 'wb') as handle:\n",
        "    pickle.dump(model, handle, protocol=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXq2gOISHW9V"
      },
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1SPbJN8HW9V",
        "outputId": "fca49748-81cd-4497-d56e-651af119d41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence: you are such a fuckerüò†\n"
          ]
        }
      ],
      "source": [
        "sentence = input(\"sentence: \")\n",
        "def clean_text(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('(@[^\\s]+)|(#[^\\s]+)', '', sentence)\n",
        "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',sentence)\n",
        "    sentence = re.sub(r'[ ]+', ' ', sentence)\n",
        "    words = word_tokenize(sentence)\n",
        "    words = [word for word in words if not word in STOPWORDS]\n",
        "    clean_sentence=[]\n",
        "    for word in words:\n",
        "        clean_sentence.append(\" \")\n",
        "    return \"\".join(clean_sentence)\n",
        "clean_text(sentence)\n",
        "sentence=emoticon(sentence)\n",
        "stemmed = stemSentence(sentence)\n",
        "sentence=[stemmed]\n",
        "sentence = tfidftrans.transform(sentence)\n",
        "#print('normal' if lr.predict(sentence)==1 else 'hateful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "E9K-hjAaHW9W"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 200\n",
        "trunc_type='post'\n",
        "padding_type='post' \n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 100000\n",
        "total_size = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7Y2LCzV2HW9W"
      },
      "outputs": [],
      "source": [
        "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(X,y, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCHvpMqYHW9W",
        "outputId": "abf5ec5f-914f-431b-e1ed-4f7afa09f5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mRtkUyUsHW9X"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(LSTM(9)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Sewu9ppmHW9X"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWgyA3ZTHW9X",
        "outputId": "c3e311d9-7419-4eb2-b121-09707c0364ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2480/2480 [==============================] - 309s 124ms/step - loss: 0.2545 - accuracy: 0.9218 - val_loss: 0.1789 - val_accuracy: 0.9436\n",
            "Epoch 2/5\n",
            "2480/2480 [==============================] - 292s 118ms/step - loss: 0.1858 - accuracy: 0.9469 - val_loss: 0.1791 - val_accuracy: 0.9438\n",
            "Epoch 3/5\n",
            "2480/2480 [==============================] - 292s 118ms/step - loss: 0.1651 - accuracy: 0.9513 - val_loss: 0.1867 - val_accuracy: 0.9392\n",
            "Epoch 4/5\n",
            "2480/2480 [==============================] - 292s 118ms/step - loss: 0.1489 - accuracy: 0.9559 - val_loss: 0.1962 - val_accuracy: 0.9385\n",
            "Epoch 5/5\n",
            "2480/2480 [==============================] - 290s 117ms/step - loss: 0.1334 - accuracy: 0.9599 - val_loss: 0.2053 - val_accuracy: 0.9379\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(training_padded, training_labels, \n",
        "                    epochs=5, \n",
        "                    validation_data=(testing_padded, testing_labels), \n",
        "                    verbose=1,callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxZBQgtjHW9X",
        "outputId": "439a33a6-178b-4b26-9acd-9e7cb43b662c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "model.save('lstm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DAeEJYY9HW9Y"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "classifier= keras.models.load_model('lstm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y7lnJarHW9Y",
        "outputId": "0c8045a7-4117-42ec-90ce-a4e36a349bf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x7fec512d76d0>>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "classifier.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "82mBatumHW9Y"
      },
      "outputs": [],
      "source": [
        "def result(sentence):\n",
        "    def clean_text(sentence):\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub('(@[^\\s]+)|(#[^\\s]+)', '', sentence)\n",
        "        sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',sentence)\n",
        "        sentence = re.sub(r'[ ]+', ' ', sentence)\n",
        "        words = word_tokenize(sentence)\n",
        "        words = [word for word in words if not word in STOPWORDS]\n",
        "        clean_sentence=[]\n",
        "        for word in words:\n",
        "            clean_sentence.append(\" \")\n",
        "        return \"\".join(clean_sentence)\n",
        "    clean_text(sentence)\n",
        "    sentence=emoticon(sentence)\n",
        "    stemmed = stemSentence(sentence)\n",
        "    sentence = tokenizer.texts_to_sequences([sentence])\n",
        "    sentence = pad_sequences(sentence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "    print(\"Probability of sentence being normal ->\")\n",
        "    print(classifier.predict(sentence))\n",
        "    print(\"Hateful or normal->\")\n",
        "    print('normal' if classifier.predict(sentence)>0.5 else 'hateful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKPLYYSyHW9Y",
        "outputId": "0f46b549-b213-406a-f4b5-da9dbde630cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nigga\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "ion want no nigga that got ties with somebody else , she can have you. ian nobody option ü•±.\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[0.00299661]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Y‚Äôall be miserable &amp; unhappy with y‚Äôall life.. &amp; wonder why y‚Äôall niggas don‚Äôt want y‚Äôall üòÇüòÇüòÇ\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "[[0.7056338]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "it‚Äôs never beef about no nigga on my end. No nigga. I promise.\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[[0.01535175]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "these niggas are ridiculous. https://t.co/Oow08MVFhL\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.7725689]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "i really wish bitches start saying they like slow niggas and leave da whole hood out it\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.9564457]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Damn what y‚Äôall Niggas be wanting besides to fuck ? lol\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[[0.04865433]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "I like being single. Ion like nun of these niggas\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[[0.9494622]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "The amount of niggas u be curving for a fuck nigga be CRAZY\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "[[0.00085677]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "im the bitch a nigga gon love Forever even if he hates me a littleü§£ü§£\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[0.03112562]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "A friendly nigga could never b a nigga of mines üò≠üò≠üò≠üòÇ\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.00077989]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "How u make a nigga leave u alone for real????\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[[0.01517066]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "I would hate to be like you niggas\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.38815227]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Y‚Äôall be talmbout studs this and dykes that. Do y‚Äôall like trans niggas? Asking for myself\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.91833955]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "every real nigga eat pussy from the back\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.00692164]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Having a nigga reach me mentally is rare. That‚Äôs what i crave for.\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[0.9409079]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Ladies this how we shutting niggas up all 2023üòÇ https://t.co/A2o8fVJLro\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.98225015]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "He really pissed me off so I added tax https://t.co/FnOp5S9ACS\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[0.8132529]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "leave that nigga where you met em\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "[[0.01538227]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "quick to give a nigga back to them lame ass hoes he love so much\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.00193891]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hateful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "Make peace with everyone,this life is nothing .\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.97958183]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "normal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Tweet->\n",
            "i would shut up if a nigga was doing what he suppose to.\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[[0.07392156]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "hateful\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import sys\n",
        "\n",
        "header = {\n",
        "\t'authorization': ('Bearer AAAAAAAAAAAAAAAAAAAAA'\n",
        "                        'NRILgAAAAAAnNwIzUejRCOuH5E6I8xnZ'\n",
        "                        'z4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA'),\n",
        "\t'x-twitter-client-language': 'en'\n",
        "\t}\n",
        "\n",
        "# FORGING GUEST TOKEN\n",
        "res = requests.post('https://api.twitter.com/1.1/guest/activate.json',headers=header)\n",
        "guest_token = res.json()['guest_token']\n",
        "header['x-guest-token'] = guest_token\n",
        "\n",
        "# PERFORMING SEARCH\n",
        "query = input()\n",
        "url = f'https://twitter.com/i/api/2/search/adaptive.json?simple_quoted_tweet=true&q={query}&count=20&query_source=typed_query'\n",
        "res = requests.get(url, headers=header)\n",
        "\n",
        "# PRINTING TWEETS\n",
        "for tweet in res.json().get('globalObjects').get('tweets').values():\n",
        "    print('-' * 500) \n",
        "    print(\"Tweet->\")\n",
        "    print(tweet.get('text'))\n",
        "    result(tweet.get('text'))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBEnlTRHW9Z",
        "outputId": "3b8e731a-6b1f-4bcf-e20e-40aad43e3bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are such a fuckerüò†\n",
            "Probability of sentence being normal ->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[[0.00208407]]\n",
            "Hateful or normal->\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "hateful\n"
          ]
        }
      ],
      "source": [
        "s=input()\n",
        "result(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbqfUuDnHW9Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}